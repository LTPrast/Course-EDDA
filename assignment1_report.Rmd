---
title: "Assignment 1"
author: 'Group 67: Jamie Faber, Laurens Prast, Amal Salman'
date: "27 February 2022"
output:
  pdf_document: default
  html_document:
    df_print: paged
highlight: tango
---
```{r include = FALSE}
options(digits=3)
options(knitr.duplicate.label = "allow")
```
```{r}

```

## Imports
```{r,include = FALSE, warning=FALSE, message=FALSE}
library("ggpubr")
library("pracma")
library("lme4")
library("dplyr")
library("car")
```


```{r include = FALSE}
# to control output:
# https://community.rstudio.com/t/showing-only-the-first-few-lines-of-the-results-of-a-code-chunk/6963/2
library(knitr)
hook_output <- knit_hooks$get("output")
knit_hooks$set(output = function(x, options) {
  lines <- options$output.lines
  if (is.null(lines)) {
    return(hook_output(x, options))  # pass to default hook
  }
  x <- unlist(strsplit(x, "\n"))
  # more <- "..."
  if (length(lines)==1) {        # first n lines
    if (length(x) > lines) {
      # truncate the output, but add ....
      x <- c(head(x, lines))
    }
  } else {
    x <- c(x[lines])
  }
  # paste these lines together
  x <- paste(c(x, ""), collapse = "\n")
  hook_output(x, options)
})
``` 

## Exercise 1
***a)***
**Checking normality of the data:**

```{r, fig.height=3, echo=-1}
data = c(15.4, 17.9, 19.0, 0.5, 15.9, 2.7, 6.2, 2.5, 4.7, 6.9, 10.8, 24.3, 5.6, 23.0, 10.7)
par(mfrow=c(1,2))
qqnorm(data);qqline(data); hist(data)
```
```{r, output.lines=-(1:4)}
shapiro.test(data);
```
Due to small sample size the plots vary more, however they don't show a clear non normal distribution nor does Shapiro's test find significance. P-value larger than confidence level: 0.05, therefore can state that there exists no significant departure from a normal distribution in the data set.


**Constructing a 97%-CI for $\mu$:**
```{r}
mean_data = mean(data)
std_data = sd(data)
n = length(data)

error = qnorm(0.985)*std_data/sqrt(n)
left = mean_data-error
right = mean_data+error

c(left, mean_data, right)     #mean and confidence interval
```
0.985 is used since 2-sided and confidence interval of 0.97. Normal distribution is used since we can assume normality.


**Evaluating the sample size needed to provide that the length of the 97%-CI is at most 2:**
```{r}
E = 1
min_sample_size = (qnorm(0.985)^2*std_data^2)/ E^2
min_sample_size
```
2E=2 so E=1. And normal distribution is used since we can assume normality. Therefore, the sample size needed is estimated to be 281.

**Computing a bootstrap 97%-CI for $\mu$:**
```{r}
B=1000
Tstar=numeric(B)
for(i in 1:B) {
  Xstar=sample(data,replace=TRUE)
  Tstar[i]=mean(Xstar) }

Tstar15=quantile(Tstar,0.015)
Tstar985=quantile(Tstar,0.985)
```
If we have a (small) sample from an unknown distribution and the distribution of XÂ¯ is not close to normal, we cannot rely on the above (asympt.) normal CI. and we can use a bootstrap method.

**Comparing t-test and bootstrap method:**
```{r}
c(left, mean_data, right) 
c(2*mean_data-Tstar985,mean(Tstar),2*mean_data-Tstar15)
```
The bootstrap method has a smaller 97%-confidence interval compared to the method were we use the normal distribution.

***b)***
**T-test to verify the calim that the mean waiting time is less than 15 minutes**

Assuming normality we can do the following t-test. We can state that there exists no significant departure from normality according to the Shapiro test from part (a).
```{r, output.lines=-(1:4)}
mu0 = 15
t.test(data,mu=mu0,alt="less")
```
Since p-value < 0.05 we reject H0, which was that H0 is equal to or larger than 15, and therefore accept H1 (that mean < 15). We can see that the 95% confidence interval for the population mean is [-inf;14.59] which means that at a significance level alpha = 0.05 we reject null hypothesis as long as the hypothesized value mu0 is below 14.59, otherwise the null hypothesis cannot be rejected.

**Sign tests:**

Binomial sign test:
```{r, output.lines=-(1:4)}
larger_than_mu = sum(data>mu0)
samples = length(data)
binom.test(larger_than_mu,samples,p=0.5, alt='less')
```
Since p-value > 0.05 we do not reject H0. We do not have sufficient evidence to say that the  patients are less likely to wait less than 15 min than more than 15 min with the binomial sign test.

```{r fig.height=3,fig.width=3,fig.align='center'}
boxplot(data, main="Boxplot",
        ylab="time (mins)")
```

While the histogram in a) does not seem symmetric, the above boxplot does. Also since a normal distribution is symmetric, which was found in a), the 
Wilcoxon signed rank test can be used. Although since the assumption of normality is not violated it makes more sense to use a parametric test.

***c)***
**Computing the powers of the t-test and sign test from b) at $\mu$ = 14 and $\mu$ = 13:**

Power test for sign test and t-test with mu0=14:
```{r}
mu0_1= 13
psign_13=numeric(B) ## will contain p-values of sign test
pttest_13=numeric(B) ## will contain p-values of t-test

for(i in 1:B) {
   x=sample(data,replace=TRUE)
   pttest_13[i]=t.test(x,mu=mu0_1,alt="less")[[3]] ## extract p-value
   psign_13[i]=binom.test(sum(x>mu0),length(data),p=0.5, alt="less")[[3]]} ## extract p-value

#percentage of t test/sign tests that are lower than 0.05. T tests have a lot
#larger percentage of tests that reject the H0.
sum(psign_13<0.05)/B
sum(pttest_13<0.05)/B

#power test for sign test and t-test with mu0=14
mu0_2 = 14
psign_14=numeric(B) ## will contain p-values of sign test
pttest_14=numeric(B) ## will contain p-values of t-test

for(i in 1:B) {
  x=sample(data,replace=TRUE)
  pttest_14[i]=t.test(x,mu=mu0_2,alt="less")[[3]] ## extract p-value
  psign_14[i]=binom.test(sum(x>mu0),length(data),p=0.5, alt="less")[[3]]} ## extract p-value

#percentage of t test/sign tests that are lower than 0.05. T tests have a lot
#larger percentage of test than reject the H0.
sum(psign_14<0.05)/B
sum(pttest_14<0.05)/B
```
The t-test rejects the H0 more than the sign test for both $\mu$ = 13 and $\mu$ = 14, which means that the wit the t-test the H1 is accepted more which was that mean < 15. What is also clearly seen for the t-test is the H1 is accepted more when $\mu$ = 14 compared to $\mu$ = 13. This is not the case for the sign test were H1 is accepted roughly the same amount.

***d)***
**Recovering the whole confidence interval and its confidence level:**
```{r}
s = sd(data)
n = length(data)

# z-score is probability
pr = 0.53 # right end confidence interval
x <- sum(data>15.5)/n # sum of values > 15.5 / total
m <- pr-x # cause pr = x+m, m is error margin
pl <- x-m # left end confidence interval
cv <- (sqrt(n)*m)/s # cause m = cv*s/sqrt(n), is critical value
area <- 0.5398 # for cv 0.10 read from z-score table
alpha <- (1-(area/2)) # don't actually need alpha, cl is area/2
cl <- (1-alpha)*100 # confidence level
cbind(pl,pr,cl)
```
Due to asymptotic normality the Z-score is used. Thus, the confidence interval is [0.137, 0.53] and its confidence level is 26.99%.

***e)***
**Verifying the claim that the waiting time is different for men and women using the Fisher exact test:**

```{r}
tab = matrix(c(3,2,4,6), ncol=2, byrow=TRUE)
colnames(tab) = c('men', 'women')
rownames(tab) = c('more than 15.5', 'less than 15.5')
tab <- as.table(tab)
tab
```
```{r, output.lines=-(1:4), warning=FALSE}
fisher.test(tab)
```
Due to the small sample size Fisher's test is used. As the p-value 0.6084 is greater than the .05 significance level, we do not reject the null hypothesis that the waiting time is below or above 15.5 min is independent of the fact that if you are a man or woman. So the claim of the researcher that the waiting time is different for men and women is not proven  based on the Fisher's test.

## Exercise 2
***a)***
**Testing whether silver nitrate has an effect:**
Plots:
```{r, include=FALSE}
clouds=read.table(file="clouds.txt",header=TRUE)
clouds
```
```{r echo=-1, fig.height=3}
par(mfrow=c(1,2))
qqnorm(clouds$seeded);qqnorm(clouds$unseeded)
```
```{r echo=-1, fig.height=5}
par(mfrow=c(2,2))
hist(clouds$seeded);hist(clouds$unseeded)
boxplot(clouds$seeded,main='Boxplot seeded');boxplot(clouds$unseeded,main='Boxplot unseeded')
```
Normality test:
```{r, output.lines=-(1:4)}
shapiro.test(clouds$seeded)
```
```{r, output.lines=-(1:4)}
shapiro.test(clouds$unseeded)
```
Plots show a very not normal data distribution which is confirmed with Shapiro's test. Box plots show the data consists of very large outliers. 

The seeded and unseeded clouds data set is not paired data. In the experiment design it is described as out of a sample of 52 clouds 26 are chosen, this already implies the clouds are separate 'individuals'. Not only that, clouds are unique and when it rains the cloud disappears (at least changes), which makes it impossible to use the same cloud twice. So we use an unpaired two samples t-test. Since we are investigating whether the rainfall increases with silver nitrate we test one sided.
```{r, output.lines=5:6}
t.test(clouds$seeded,clouds$unseeded,alternative = "greater")
```

Mann-Whitney test:
```{r, output.lines=5:6, warning=FALSE}
wilcox.test(clouds$seeded,clouds$unseeded,alternative = "greater")
```

Kolmogorov-Smirnov test:
```{r, output.lines=5:6, warning=FALSE}
ks.test(clouds$unseeded,clouds$seeded,alternative = "greater")
```

Are tests applicable?
Mann-Whitney and Kolmogorov need the samples to be independent. Due to the unique nature of clouds and nothing in the experiment design suggests this assumption can't be withheld. 
T-test isn't applicable due to assumption of normality being violated.

Non parametric tests are more significant due to outliers for which these tests aren't as sensitive for. Mann-Whitney is a rank test (median) and Kolmogorov checks the differences of the verticality of the histograms, since outliers are infrequent, verticality of histogram for the outlier is small.

***b)***
**Repeat (a) with square root transformation:**

```{r echo=-3, fig.height=5}
sqrtseeded <- sqrt(clouds$seeded); sqrtunseeded <- sqrt(clouds$unseeded)
# Plots:
par(mfrow=c(2,2))
qqnorm(sqrtseeded);qqnorm(sqrtunseeded)
hist(sqrtseeded);hist(sqrtunseeded)
```
```{r echo=-3, fig.height=3}
par(mfrow=c(1,2))
boxplot(sqrtseeded,main='Boxplot sqrtseeded');boxplot(sqrtunseeded,main='Boxplot sqrtunseeded')
```

Normality test:
```{r, output.lines=-(1:4)}
shapiro.test(sqrtseeded)
```
```{r, output.lines=-(1:4)}
shapiro.test(sqrtunseeded)
```
An improvement can be seen in the plots and Shapiro test. However the data still does not have a normal distribution and still consists of outliers.

Two samples t-test:
```{r, output.lines=5:6}
t.test(sqrtseeded,sqrtunseeded,alternative = "greater")
```
Mann-Whitney test:
```{r, output.lines=5:6, warning=FALSE}
wilcox.test(sqrtseeded,sqrtunseeded,alternative = "greater")
```
Kolmogorov-Smirnov test:
```{r, output.lines=5:6, warning=FALSE}
ks.test(sqrtunseeded,sqrtseeded,alternative = "greater")
```

While the data set still contains outliers they are less large. T-test assumptions are still violated even though the resulting p-value is more significant. The transformation doesn't affect the significance of the ranked tests nor Kolmogorov Smirnoff test since the rank and frequency of the data points is still the same.

**Repeat (a) with square root of square root transformation:**
```{r echo=-3, fig.height=6.5}
sqrtsqrtseeded <- sqrt(sqrtseeded); sqrtsqrtunseeded <- sqrt(sqrtunseeded)
# Plots:
par(mfrow=c(3,2))
qqnorm(sqrtsqrtseeded);qqnorm(sqrtsqrtunseeded)
hist(sqrtsqrtseeded);hist(sqrtsqrtunseeded)
boxplot(sqrtsqrtseeded,main='Boxplot sqrtsqrtseeded');boxplot(sqrtsqrtunseeded,main='Boxplot sqrtsqrtunseeded')
```
Normality test:
```{r, output.lines=-(1:4)}
shapiro.test(sqrtsqrtseeded)
```
```{r, output.lines=-(1:4)}
shapiro.test(sqrtsqrtunseeded)
```
The data set is now normally distributed. Since the assumption of normality is not violated any more for the t-test, we also check to see if the variances are equal. To do this we use the F-test:

```{r, output.lines=5:6}
var.test(sqrtsqrtseeded, sqrtsqrtunseeded, alternative = "two.sided")
```
The variance between groups is insignificant and can be considered equal. 

Two samples t-test:
```{r, output.lines=5:6}
t.test(sqrtsqrtseeded,sqrtsqrtunseeded, var.equal = TRUE, alternative = "greater")
```
Mann-Whitney test:
```{r, output.lines=5:6, warning=FALSE}
wilcox.test(sqrtsqrtseeded,sqrtsqrtunseeded,alternative = "greater")
```
Kolmogorov-Smirnov test:
```{r, output.lines=5:6, warning=FALSE}
ks.test(sqrtsqrtunseeded,sqrtsqrtseeded,alternative = "greater")
```
Are tests now applicable with transformations?
Mann-Whithey and Kolmogorov were applicable the whole time and now since only 1 outlier is left and the data is normal distributed the t-test assumptions are not violated. 
P-values are similar, all around 0,01.The t-test is slightly more significant, which is to be expected due the power of the t-test. With out the remaining outlier it would most likely be even more significant compared to the non parametric tests.

***c)***
**Finding an estimate of $\lambda$ and constructing a 95%-CI for  $\lambda$:**
```{r}
c1 <- clouds$seeded
lambda_est <- 1/mean(c1)
n <- length(clouds$seeded)
sd <- 1/lambda_est
# CI
z <- qnorm(1-0.025)
CI_L <- (-z+sqrt(n))/(mean(c1)*sqrt(n))
CI_R <- (z+sqrt(n))/(mean(c1)*sqrt(n))
cbind(CI_L,CI_R)
```
Estimate of lambda = 0.00226
CI around lambda: [0.00139,0.00313]


**Bootstrap test:**
```{r, fig.height=3}
t=median(c1);t
B=1000
tstar=numeric(B)
n=length(c1)
for (i in 1:B){
  xstar=rexp(n,lambda_est)
  tstar[i]=median(xstar)}
hist(tstar,prob=T)
pl=sum(tstar<t)/B; pr=sum(tstar>t)/B; p=2*min(pl,pr)
cbind(pl,pr,p)
```

**Kolmogorov-Smirnov test:**
```{r, output.lines=-(1:4), warning=FALSE}
ks.test(clouds$seeded,"pexp", lambda_est)
```
For the bootstrap the test statistic median is used. The median of our sample is 222 and as can be seen in the histogram differs from the median of the simulated data. To test whether the data follows an exp(lambda_est) distribution is significantly different: the H0 (that the data distribution is the same) cannot be rejected (both bootstrap test and Kolmogorov are insignificant). Thus we conclude that the distribution of the data follows a exp(lambda_est) distribution.
***d)***
**Verifying whether the median precipitation for seeded clouds is less than 300**

In a) we showed that the cloud data is not symmetric nor normal distributed. So we use a sign test. 

Binomial test:
```{r, output.lines=5:6}
x <- sum(clouds$seeded<300)
n <- length(clouds$seeded)
binom.test(x,n,p=0.5,alt='less')
```
Not significant (P-value = 0.962) so h0 not rejected: which is medians are the same. Thus cannot conclude median precipitation is less than 300

**Checking whether the fraction of the seeded clouds with precipitation less than 30 is at most 25%:**

Binomial test:
```{r, output.lines=5:6}
x <- sum(clouds$seeded<30)
binom.test(x,n,p=0.25,alt='less')
```
Not significant (P-value = 0.0802) h0 not rejected: which is medians are the same. Thus cannot conclude the fraction of precipitation under 30 is at most 25%.


## Exercise 3
***a)***
We cannot assume that the data was taken from normal populations because we have a very small sample size (only 10) and that is not enough to invoke the central limit theorem because we need a sample size of at least 30 to assume that. Furthermore, the distribution of the sample data is not normal as can be seen by the plot and normality test below.
```{r include=FALSE}
data=read.table(file="dogs.txt",header=TRUE)
data
```

```{r, echo=-1,fig.height=3}
par(mfrow=c(1,2))
qqnorm(data$isofluorane);qqline(data$isofluorane); hist(data$isofluorane)
```
```{r, output.lines=-(1:4)}
shapiro.test(data$isofluorane)
```

***b)***
**Checking whether the columns isofluorane and halothane are correlated:**

Using Spearmanâs rank correlation test because it does not assume normality.
```{r, output.lines=5:6, warning=FALSE}
cor.test(data$isofluorane, data$halothane, method = "spearman")
```
p-value > 0.05 so the correlation is not significant.

**Verifying whether the distributions of the isofluorane and halothane columns are different:**

The permutation test is applicable here because it doesn't assume normality and the sample size is small.
```{r, fig.height=2.4}
mystat=function(x,y) {mean(x-y)}
B=1000; tstar=numeric(B)
for (i in 1:B) {
  datastar=t(apply(cbind(data[,1],data[,2]),1,sample))
  tstar[i]=mystat(datastar[,1],datastar[,2]) }
myt=mystat(data[,1],data[,2])
myt
hist(tstar)
pl=sum(tstar<myt)/B
pr=sum(tstar>myt)/B
p=2*min(pl,pr)
p
```

p-value > 0.05 so no significant difference in the distributions of the isofluorane and halothane columns.

***c)***
**One-way ANOVA test:**
```{r, output.lines=4:5}
dataframe=data.frame(concentration=as.vector(as.matrix(data)),
                     variety=factor(rep(1:3,each=10)))
aov=lm(concentration~variety,data=dataframe)
anova(aov)
```

P-value < 0.05 so factor variety is significant. So yes, the type of drug has an effect on the concentration of plasma epinephrine.

```{r, fig.height=3}
par(mfrow=c(1,2))
qqnorm(residuals(aov)); qqline(residuals(aov))
plot(fitted(aov),residuals(aov), main = "Residual Plot")
```
```{r, output.lines=-(1:4)}
leveneTest(concentration~variety,data=dataframe)
```
The plot of the residuals shows the assumption of normailty can withhold, however the spread in the residuals suggest that there is no equal variance in the residuals. To confirm this a Levenes test was performed (P < 0.05). Thus the equal variances assumption of the one-way ANOVA test is not met.

**The estimated concentrations for each of the three anesthesia drugs:**
```{r, output.lines=10:13}
summary(aov)
```
The estimated concentrations are 0.43 for isofluorane, 0.04 for halothane, and 0.42 for cyclopropane. It's worth noting that the p-value for halothane is the only one greater than 0.05.

***d)***
**Kruskal-Wallis test:**
```{r, output.lines=-(1:4)}
attach(dataframe); kruskal.test(concentration,variety)
```
p-value > 0.05 so factor variety is not significant. So no, the two tests don't arrive at the same conclusion. While the one-way ANOVA test has more power than the Kruskal-Wallis test, it is less accurate in this situation because the assumption of equal variances is not met. The nonparametric Kruskal-Wallis test has less strict assumptions which are met and is thus more accurate.

## Exercise 4
***a)***
**Randomization process to distribute 80 fishes:**
```{r echo = TRUE, results = 'hide'}
I=4; J=2; N=10
rbind(rep(1:I,each=N*J),rep(1:J,N*I),sample(1:(N*I*J)))
```

4 rate factors 2 method factors and each specific combination is repeated 
for 10 fishes. With this code the 80 fishes are distributed randomly 
over all combinations of levels of factors rate and method.

***b)***
**Two-way ANOVA test:**
```{r include = FALSE}
data = read.table("hemoglobin.txt", header=TRUE)
```
```{r echo=-3, fig.height=3.5}
#necessary other wise rate is numerical and should be categorical
data$rate=as.factor(data$rate)
par(mfrow=c(1,2))
boxplot(hemoglobin ~ method, data = data,main = "Boxplot") ; 
boxplot(hemoglobin ~ rate, data = data,main = "Boxplot")
```
```{r, echo=-1, fig.height=3}
par(mfrow=c(1,2))
interaction.plot(x.factor = data$rate, trace.factor = data$method, response = data$hemoglobin, main="Interaction Plot",
        xlab="rate",
        ylab="hemoglobin",trace.label = "method:");
interaction.plot(x.factor = data$method, trace.factor = data$rate, response = data$hemoglobin, main="Interaction Plot",
        xlab="method",
        ylab="hemoglobin",trace.label = "rate:")
```

```{r, output.lines=4:7}
hemoglobinaov=lm(hemoglobin ~ method * rate, data = data) 
anova(hemoglobinaov)
```
The p-value for the interaction between method and the rate is larger than 0.05. So, there is no evidence for interaction between the two factors. Therefore we will create a new model without the interaction called the additive model.

```{r, output.lines=5:6}
hemoglobinaov2 =lm(hemoglobin ~ method + rate, data = data)
anova(hemoglobinaov2)
```
With the two way anova additive model we can see that the method factor had a p-value > 0.05 and therefore has no  main effect for the outcome of this research. The rate factor has a p-value < 0.05 which means that the effect of different amounts of sulfamerazine that is given to the  brown trout gives significantly different amounts of hemoglobin in their blood.

```{r, fig.height=3}
par(mfrow=c(1,2))
qqnorm(residuals(hemoglobinaov2))
qqline(residuals(hemoglobinaov2))
plot(fitted(hemoglobinaov2),residuals(hemoglobinaov2), main = "Boxplot")
hemoglobinaov2
```

```{r, output.lines=-(1:4)}
shapiro.test(residuals(hemoglobinaov2))
```
QQ plot looks like the residuals are normally distributed. For the Shapiro's test the p-value > 0.05, therefore passed the test and can state that there exists no significant departure from normality. The spread in the residuals seems to be in the similar range for all the fitted values. So, we can assume equal variance in the residuals. Because the requirements of normality and the assumption of equal variances are met, we can use the two-way ANOVA test.


***c)***
**Factor with greater influence:**
```{r, output.lines=-(1:3), message=FALSE}
group_by(data, method, rate) %>%
  summarise(
    count = n(),
    mean = mean(hemoglobin, na.rm = TRUE),
    sd = sd(hemoglobin, na.rm = TRUE)
  )
```
The rate factor has a significant main effect on the the amount of hemoglobin, while the method factor does not have a significant main effect on the amount of hemoglobin. So the rate factor has the greatest influence of the two. This is a good question, because one factor is significant and the other is not if both were insignificant than it would not be a good question.

**Combination of rate and method that yields the highest hemoglobin:**
Rate 2 and method B leads to a mean of 10.1, which is the highest hemoglobin yield when both factors are considered. When rate 3 and method A was used then the mean was estimated to be 9.03.

**Rate that leads to the highest mean hemoglobin:**
```{r, output.lines=-(1:2)}
group_by(data, rate) %>%
  summarise(
    count = n(),
    mean = mean(hemoglobin, na.rm = TRUE),
    sd = sd(hemoglobin, na.rm = TRUE)
  )
```
When only the factor rate is considered then rate 2 gives the highest mean rate with a mean of 9.74 hemoglobin yield.

***d)***
**One-way ANOVA test:**
```{r, output.lines=4:6}
hemoglobinaov3 =lm(hemoglobin ~ rate, data = data)
anova(hemoglobinaov3)
```
With the one-way anova we see that the p-value < 0.05 and therefore the rate factor gives significant different means for the hemoglobin. We can use the one-way anova test on this data set because the factor method is insignificant and the interaction between the rate factor and the method factor is also insignificant.

## Exercise 5
***a)***
**Repeated Anova without interactions:**
```{r include=FALSE}
cream <- read.table(file="cream.txt",header=TRUE)
cream
```
```{r, output.lines=4:7}
cream$batch <- factor(cream$batch)
cream$position <- factor(cream$position)
cream$starter <- factor(cream$starter)
aovcream <- lm(acidity~batch+position+starter,data=cream); anova(aovcream)
```
```{r, output.lines=10:23}
summary(aovcream)
```
```{r echo=-1, fig.height=3}
par(mfrow=c(1,2))
qqnorm(residuals(aovcream));qqline(residuals(aovcream));plot(fitted(aovcream),residuals(aovcream),main="Residual Plot")
```
```{r, output.lines=-(1:4)}
shapiro.test(residuals(aovcream));
```

Three factors are relevant to look at so a three way Anova is used. Since the same starter combination is being tested multiple times in different locations a repeated measures test is used. So the test used is a three way repeated measure Anova.
Position no significant effect (P = 0.411). However batch does have significant effect (P =0.00163) (not expected in experiment design) and starter also has a significant effect (P = 2.904e-05). 
From summary: p value starter2 not significant (p = 0.754), this is compared to starter1. Thus starter 1 and 2 do not significantly differ.
The plots and normality tests show the assumption of normality is not violated and looking at the residuals plot the assumption of homogeneity of variances is also not violated. So the chosen Anova test is applicable.


***b)***
```{r, output.lines=4:7}
aovcream=lm(acidity~batch+starter,data=cream); anova(aovcream)
```
```{r, output.lines=(9:19)}
summary(aovcream)
```
Batch is not an insignificant block variable (P = 0.000735), even though in the experiment design the batch was meant to be identical. However the position has no influence so can be considered an insignificant block element (P = 4.816e-06) and be removed. The now two-way repeated measures Anova:
Out of all the starters, only starter 4 is significant (P = 6.10e-05)

***c)***
**Friedman**
```{r, output.lines=-(1:4)}
friedman.test(acidity~batch | starter,data=cream)
```
Friedman test is a non parametric version of the repeated measures Anova. Since the the assumptions for the repeated measure Anova are met, it makes more sense to use that test, no assumptions for Friedman's test (data is independent and data can be ranked) are violated. so it is possible to use it with acidity as response values, starter as the group and batch as block variables. However to see which starter of the starters causes the significant effect, a post hoc test would need to be applied. 

***d)***
```{r, output.lines=(12:24)}
creamlmer=lmer(acidity~starter+(1|batch),REML=FALSE,data=cream)
summary(creamlmer)
```
```{r, output.lines=5:7}
creamlmer1=lmer(acidity~(1|batch),data=cream,REML=FALSE)
anova(creamlmer,creamlmer1)
```

Using the same model as in b, however now the block model batch is modeled with a random effect. Since lmer doesn't give p-values, it is compared to the same model without starters. The variance between these models gives us the significance of the starter factor. Comparing the model with random effects with the fixed effect model in b), Starter 4 has the most significant effect on acidity in both models and slightly larger t-value in lmer (T = 6.660) compared to the fixed effect model in b) (T = 5.957). Furthermore the estimates are slightly larger in the random effects model. So similar results are found, except for the difference that no estimates were given for the batch variable, since this was modeled as a random effect. Since in the experimental design the batch was not meant to have an effect, we argue that modelling the batch as a random block variable is an improvement.  
